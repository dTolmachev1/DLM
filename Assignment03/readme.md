## Где расположен код модели и кто разработчик?
    Разработчиком модели выступает команда AI Сбера (исходя из названия репозиториев и пары статей)
    Код модели можно найти на git:
        - Если клонировать -> `git clone https://github.com/sberbank-ai/sber-swap.git`
        - Если хочется открыть в браузере -> (https://github.com/ai-forever/ghost)
## Есть ли статья по работе модели? Если да, то в чем суть подхода авторов, как обучалась модель, какие метрики были достигнуты?
    Статья непосредственно самого сбера на хабре - (https://habr.com/ru/companies/sberbank/articles/575614/)
    Публикация на Ieeexplore - (https://ieeexplore.ieee.org/abstract/document/9851423)
    Суть подхода авторов аналогична FaceShifter, но еще был добавлен дискриминатор (как в SimSwap или Few-Shot Adversarial Learning of Realistic Neural Talking Head Model) и нейросеть для нахождения лица.
    Краткая суть - в модели используются под капотом 4 модели, первая используется для определения лица, вторая для вырезания лица, третья строит новую картинку по параметрам полученным ранее, а после улучшает качество результирующего изображения и четвертая проверяет результат.
### Метрики
    Касательно метрик разработчики описывают процесс обучения нейросетей с помощью других нейросетей...
    Была использована CoseFace model для установки качества распознавания "изначального" лица. Далее использовалась модель для распознавания 3D лиц Ringnet, а также pose estimator для оценки различных параметров лиц (изначального и "восстановленного")
## Состоит ли модель из нескольких частей? Если да, то из каких, что они делают и как называются в коде?
    Модель состоит из 4 частей
    - Identity encoder (предобученная модель, которая извлекает лица из изображения)
    - Attribute encoder (извлечение параметров с лиц)
    - AAD Generator ("восстановление" нового изображения по сетке параметров)
    - Discriminator (валидация)
## Как проходит процесс инференса модели для фото и видео (подробно)? Какие применяются преобразования входных и выходных данных?
    В статье описан стандартный pipeline. В целом он схож с FaceShifter, но есть некоторые отличия.
    Для извлечения лица из изображения используется SCRFD по умолчанию. В теории он позволяет извлекать много лиц из изображения. После чего образуется маска лица, в данном случае - маска сегментация. Она помечает какие пиксели относятся к лицу, а какие нет (бинарное изображение). После этого происходит кроп изображения. Чтобы получившийся кроп был похож на нормальный результат, используется размытие по Гауссу по краям
    Когда все грани (величины вектора кропа) были обнаружены его можно сгладить, используя информацию соседних кадров в случае видео
    Далее модель корректирует результат путём изменения размера маски (если было сильно много, то зацепилось не только лицо, но и форма головы, если было мало - то получится слишком большой кроп(тут crop - увеличение))
    После этого запускается Super Resolution Post-Processing - нейросеть для улучшения качества изображения
    Для инференса всё работает также, только всё прогоняется для всех кадров в видео
## Можно ли дообучить модель и какие для этого требуются ресурсы? Может, авторы давали советы по обучению/дообучению?
    Сами разработчики дают комментарий на тему, как будут дальше улучшать модель, но пользователи для дообучения модели могут поэкспериментировать с Loss функцией, и количеством блоков в генераторе. Авторы же пишут следующее:
    "С точки зрения будущих исследований мы рассматриваем процесс тонкой настройки нашего решения в конвейере GAN, где предлагаемая архитектура будет использоваться в качестве генератора, а модели глубокого обнаружения подделок SoTA будут использоваться в качестве дискриминатора. Мы ожидаем получить эффект качества для общих и сложных случаев (условия освещения, экстремальное положение головы и т.д.). Другие будущие задачи касаются добавления функциональности конкретных атрибутов в процесс замены лиц и внедрения функции CLIP-like (контрастный язык-предварительная подготовка изображения) в наш конвейер для управления процессом замены лиц с помощью некоторой текстовой подсказки (например, используя текстовую подсказку “сделать губы красными”, мы можем получить результат смешивания с красными губами, даже если они не были красными на исходном изображении)."